### RAG-i1 is a state-of-the-art Retrieval-Augmented Generation (RAG) system designed for precise and efficient information retrieval and response generation. This project integrates advanced techniques for document chunking, embedding-based retrieval, and LLM-based response generation, creating a robust solution for knowledge-driven tasks.

### Key Features:
  1. Document Ingestion: Efficiently processes and chunks large datasets for easy retrieval.
  2. Advanced Embeddings: Uses nomic-embed-text for high-quality vector embeddings of documents.
  3. Chroma Vector Store: A scalable solution for storing and querying document vectors.
  4. Dynamic Query Expansion: Enhances retrieval quality through context-aware query handling.
  5. LLM Integration: Leverages Ollama models to generate precise, contextually relevant answers.
  6. Evaluation Metrics: Includes recall, MAP (mean average precision), and exact match for rigorous system performance evaluation.

### Technologies:
  Python | Chroma DB | Ollama Models | Gradio Interface
  Embedding model: nomic-embed-text
  Generation model: Ollama Mistral, llama 3.1, phi3.5
### Use Cases:
  Enterprise knowledge management
  Intelligent document retrieval and summarization
  Dynamic question-answering systems
  
Cogniflow-i1 is perfect for organizations looking to leverage cutting-edge RAG systems for tasks like data analysis, report generation, and knowledge management. The project is modular, scalable, and designed with production-readiness in mind.
### References (Research Papers)
- [Expand, Rerank, and Retrieve: Query Reranking for Open-Domain Question Answering][url](https://arxiv.org/pdf/2305.17080)
- [Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity][url](https://github.com/starsuzi/Adaptive-RAG)
- [SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION][url](https://arxiv.org/pdf/2310.11511)

