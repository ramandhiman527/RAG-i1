import argparse
from langchain.prompts import ChatPromptTemplate
from langchain_community.llms import Ollama
from langchain_chroma import Chroma
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import EmbeddingsFilter
from get_embedding_function import get_embedding_function
from pprint import pprint

CHROMA_PATH = "chroma"

# Updated Few-shot Prompt Template with examples
FEW_SHOT_PROMPT_TEMPLATE = """
                            You are a helpful assistant that answers questions based on the given context.

                            Here's an example of how to answer a question based on a context:

                            Context: The ship encountered an electrical failure on 01/01/2024 and the issue was resolved on 05/01/2024 by replacing the Magnetron.
                            Question: When did the defect occur?
                            Answer: The defect occurred on 01/01/2024.

                            Now, I'll provide you with a new context and question. Please answer the question based on the context provided.

                            Context: The repair of System 1 was successfully completed by Ram Vilas from Repair Unit 1.
                            Question: Who resolved the defect?
                            Answer: The defect was resolved by Ram Vilas from Repair Unit 1.

                            Now, here's your new context and question:

                            Context: {context}
                            Question: {question}

                            Answer based on the context:

                            The answer will be generated by the language model based on the context and the question. It might look something like this:

                            Answer: Based on the context, the defect was resolved by [name] from [repair unit].
                            """

def main():
    query_text = "When did the defect occur?"
    print("Query text:", query_text)
    data = query_rag(query_text)
    return data

def query_rag(query_text: str):
    # Step 1: Expand the query using the Ollama model
    expanded_query = expand_query(query_text)

    # Step 2: Prepare the context
    embedding_function = get_embedding_function()
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)
    
    # Step 3: Retrieve documents based on the expanded query (base retrieval)
    retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 1})
    context_docs = retriever.get_relevant_documents(expanded_query)
    
    # Step 4: Contextual compression with embeddings filter
    embeddings_filter = EmbeddingsFilter(embeddings=embedding_function, similarity_threshold=0.5)
    compression_retriever = ContextualCompressionRetriever(base_compressor=embeddings_filter, base_retriever=retriever)
    
    # Retrieve relevant documents based on the compressed context
    compressed_docs = compression_retriever.get_relevant_documents(expanded_query)
    
    # Step 5: Retrieve documents based on the original query (direct retrieval)
    query_retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 5})
    query_docs = query_retriever.get_relevant_documents(query_text)
    
    # Step 6: Intersect the retrieved document IDs
    context_doc_ids = {doc.metadata["id"] for doc in context_docs}
    query_doc_ids = {doc.metadata["id"] for doc in query_docs}
    intersected_doc_ids = context_doc_ids.intersection(query_doc_ids)
    
    # Step 7: Re-rank the relevant documents based on the query
    final_retrieved_docs = [
        doc for doc in compressed_docs if doc.metadata["id"] in intersected_doc_ids
    ]
    
    # Step 8: Prepare the context from the retrieved and compressed documents
    context_text = "\n\n---\n\n".join([doc.page_content for doc in final_retrieved_docs])
    
    # Dynamic few-shot prompting with context
    prompt_template = ChatPromptTemplate.from_template(FEW_SHOT_PROMPT_TEMPLATE)
    prompt = prompt_template.format(context=context_text, question=query_text)
    
    # Step 9: Use Ollama model for generating a response
    model = Ollama(model="mistral")
    response_text = model.invoke(prompt)
    
    # Prepare the sources for output
    sources = [doc.metadata.get("id", None) for doc in final_retrieved_docs]
    data = {
        "query_text": query_text,
        "sources": sources,
        "Response": response_text.strip()
    }
    return data

def expand_query(query_text: str) -> str:
    """
    Expand the query using the Ollama model to generate synonyms and related terms.
    """
    # Use the Ollama model to generate query expansions
    expansion_prompt = f"Generate a list of synonyms and related terms for the following query: '{query_text}'. Provide the terms separated by commas."
    model = Ollama(model="phi3.5")
    expansion_response = model.invoke(expansion_prompt)
    # Extract the terms from the response
    expanded_terms = expansion_response.strip().split(',')
    expanded_terms = [term.strip() for term in expanded_terms if term.strip()]
    expanded_query = query_text + " " + " ".join(expanded_terms)
    
    print(f"Expanded Query: {expanded_query}")
    return expanded_query

if __name__ == "__main__":
    pprint(main())